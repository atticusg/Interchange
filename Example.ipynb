{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from modeling.pretrained_bert import PretrainedBertModule\n",
    "from modeling.lstm import LSTMModule\n",
    "from trainer import load_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load in the model. Note that load_model returns a tuple (<model>, <checkpoint>).\n",
    "# The checkpoint just contains information and cofigurations for training, and isn't very relevant in our experiments.\n",
    "bert_model_path = \"mqnli_models/bert-easy-best.pt\"\n",
    "lstm_model_path = \"mqnli_models/lstm-easy-best.pt\"\n",
    "\n",
    "bert_model, _ = load_model(PretrainedBertModule, bert_model_path, device=device)\n",
    "lstm_model, _ = load_model(LSTMModule, lstm_model_path, device=device)\n",
    "\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval() # disable dropout\n",
    "lstm_model = lstm_model.to(device)\n",
    "lstm_model.eval() # disable dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# pytorch checkpoint containing preprocessed data\n",
    "bert_easy_data_path = \"mqnli_models/bert-preprocessed-data.pt\"\n",
    "\n",
    "# load the data\n",
    "bert_data = torch.load(bert_easy_data_path)\n",
    "print(type(bert_data)) # datasets.mqnli.MQNLIBertData\n",
    "\n",
    "# An MQNLIBertData object contains the train, dev, and test sets, and other tools and data structures for tokenization.\n",
    "\n",
    "bert_train_set = bert_data.train\n",
    "bert_dev_set = bert_data.dev\n",
    "bert_test_set = bert_data.test\n",
    "\n",
    "print(type(bert_dev_set)) # datasets.mqnli.MQNLIBertDataset\n",
    "\n",
    "# The MQNLIBertDataset (not MQNLIBertData) class inherits the torch.utils.data.Dataset class and uses pytorch's Dataset interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through examples (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing first ten examples in the dev set\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(bert_dev_set, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# Iterating through the dataloader produces batches of examples.\n",
    "# Each batch of examples is packaged into a tuple/list that is fed into the model.\n",
    "# The tuple has the form:\n",
    "# (\n",
    "#   input_ids,       # numeric token ids for the sentence itself. shape=(batch_size, 27)\n",
    "#   token_type_ids,  # For bert: A 0/1 tag for each token indicating if it is the premise (0) or hypothesis(1). shape=(batch_size, 27)\n",
    "#   attention_masks, # For bert: A 0./1. float value to indicate if its a padding token that shouldn't be attended to (0). shape=(batch_size, 27)\n",
    "#   original_input,  # numeric token ids for the input, but without the [CLS] and [SEP] tokens and\n",
    "#                      without breaking up doesnot, notevery into two words. shape=(batch_size,18)\n",
    "#   label,           # gold label, 0 (neutral), 1 (entailment), 2 (contradiction). shape=(batchsize,)\n",
    "# )\n",
    " # the function that generates this tuple is datasets.mqnli.MQNLIBertDataset.__getitem__()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, input_tuple in enumerate(dataloader):\n",
    "        if i == 10: break\n",
    "        input_tuple = [x.to(device) for x in input_tuple]\n",
    "        labels = input_tuple[-1]\n",
    "\n",
    "        logits = bert_model(input_tuple) # call the forward function of the Bert model\n",
    "        pred = torch.argmax(logits, dim=1) # get label predictions\n",
    "        print(pred == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (LSTM)\n",
    "\n",
    "This part is pretty much the same as Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch checkpoint containing preprocessed data\n",
    "lstm_easy_data_path = \"mqnli_models/lstm-preprocessed-data.pt\"\n",
    "\n",
    "# load the data\n",
    "lstm_data = torch.load(lstm_easy_data_path)\n",
    "print(type(lstm_data)) # datasets.mqnli.MQNLIData\n",
    "\n",
    "# An MQNLIData object contains the train, dev, and test sets, and other tools and data structures for tokenization. This is all same as\n",
    "\n",
    "lstm_train_set = lstm_data.train\n",
    "lstm_dev_set = lstm_data.dev\n",
    "lstm_test_set = lstm_data.test\n",
    "\n",
    "print(type(lstm_dev_set)) # datasets.mqnli.MQNLIDataset\n",
    "# up to now this is same as loading Bert Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through examples (LSTM)\n",
    "\n",
    "This part is slightly different. The LSTM model only accepts inputs where the first dimension (dim=0) is sentence length, and the second dimension (dim=1) is the batch size. To do this we need a special `collate_fn` that does this transposition, as the `DataLoader` outputs batch_first by default. Other than that the rest remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing first ten examples in the dev set\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.mqnli import get_collate_fxn\n",
    "\n",
    "collate_fn = get_collate_fxn(lstm_dev_set, batch_first=False) # get the collate function automatically\n",
    "dataloader = DataLoader(lstm_dev_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Iterating through the dataloader produces batches of examples.\n",
    "# Each batch of examples is packaged into a tuple/list that is fed into the model.\n",
    "# The tuple has the form:\n",
    "# (\n",
    "#   input_ids,       # numeric token ids for the sentence itself. ***shape=(19, batch_size)*** where 19 is sentence length, and contains a [SEP] token.\n",
    "#   label,           # gold label, 0 (neutral), 1 (entailment), 2 (contradiction). shape=(batchsize,)\n",
    "# )\n",
    "# the function that generates this tuple is datasets.mqnli.MQNLIDataset.__getitem__()\n",
    "\n",
    "# this part is same as before\n",
    "with torch.no_grad():\n",
    "    for i, input_tuple in enumerate(dataloader):\n",
    "        if i == 10: break\n",
    "        input_tuple = [x.to(device) for x in input_tuple]\n",
    "        print(len(input_tuple))\n",
    "        labels = input_tuple[-1]\n",
    "\n",
    "        logits = lstm_model(input_tuple) # call the forward function of the LSTM model\n",
    "        pred = torch.argmax(logits, dim=1) # get label predictions\n",
    "        print(pred == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tuple[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
