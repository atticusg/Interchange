import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim# TODO: update LSTM to output logitsclass LSTMModule(nn.Module):    def __init__(self, embed_dim=40, lstm_hidden_dim=20, vocab_size=10000,                 bidirectional=True, num_lstm_layers=1, device=None, embed_init_scaling=0.1):        super(LSTMModule, self).__init__()        self.embed_dim = embed_dim        self.lstm_hidden_dim = lstm_hidden_dim        self.vocab_size = vocab_size        self.bidirectional = bidirectional        self.num_lstm_layers = num_lstm_layers        self.embed_init_scaling = embed_init_scaling        self.layer_hidden_dim = lstm_hidden_dim * 2 if bidirectional else \            lstm_hidden_dim        self.device = device if device else torch.device("cpu")        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)        self.embed.weight = nn.Parameter(torch.mul(embed_init_scaling, self.embed.weight), requires_grad=True)        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True,                            bidirectional=bidirectional,                            num_layers=num_lstm_layers)        self.mean = MeanModule()        self.logits = nn.Linear(self.layer_hidden_dim, 1)        self.sigmoid = nn.Sigmoid()    def config(self):        return {            "embed_dim": self.embed_dim,            "lstm_hidden_dim": self.lstm_hidden_dim,            "vocab_size": self.vocab_size,            "bidirectional": self.bidirectional,            "num_lstm_layers": self.num_lstm_layers,            "embed_init_scaling": self.embed_init_scaling        }    def forward(self, input_tuple, verbose=False):        x_batch, x_lengths = input_tuple[0], input_tuple[2]        emb_x = self.embed(x_batch)        hidden = self._run_lstm(emb_x, x_lengths)        h = self.mean(hidden, x_lengths)        x = self.logits(h)        x = self.sigmoid(x).squeeze()        pred = torch.round(x).type(torch.int)        return x, pred    def _run_lstm(self, emb_x, x_lengths):        x = nn.utils.rnn.pack_padded_sequence(emb_x, x_lengths,                                              batch_first=True)        x, _ = self.lstm(x)        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)        return xclass SelfAttentionModule(nn.Module):    def __init__(self, vocab_size=10000, attn_query_dim=5, hidden_dim=40,                 attn_value_dim=5):        super(SelfAttentionModule, self).__init__()        self.query_embed = nn.Embedding(vocab_size, attn_query_dim, padding_idx=0)        self.key_lin_trans = nn.Linear(hidden_dim, attn_query_dim)        self.value_lin_trans = nn.Linear(hidden_dim, attn_value_dim, bias=False)    def forward(self, x, x_batch):        """ Self attention module        :param keys: Hidden vectors        :param x_batch: Raw tokens in idx form        :return:        """        queries = self.query_embed(x_batch)        keys = self.key_lin_trans(x)        a = torch.matmul(queries, torch.transpose(keys, 1, 2))        s = torch.sum(a, 1)        # print("queries:",queries.shape,"\nkeys:", keys.shape, "\na:", a.shape,        #       "\ns:", s.shape)        padding_mask = torch.ne(x_batch, 0).type(torch.float)        exp = torch.mul(torch.exp(s), padding_mask)        attn = torch.nn.functional.normalize(exp, p=1, dim=1).unsqueeze(2)        values = self.value_lin_trans(x)        # print("attn:", attn.shape, "\nvalues:", values.shape)        return torch.sum(attn * values, 1)class LSTMSelfAttnModule(LSTMModule):    def __init__(self, embed_dim=40, lstm_hidden_dim=20, vocab_size=10000,                 bidirectional=True, num_lstm_layers=1, attn_query_dim=5,                 attn_value_dim=5, device=None):        super(LSTMSelfAttnModule, self).__init__(            embed_dim=embed_dim, lstm_hidden_dim=lstm_hidden_dim,            vocab_size=vocab_size, bidirectional=bidirectional,            num_lstm_layers=num_lstm_layers, device=device)        self.attn_query_dim = attn_query_dim        self.attn_value_dim = attn_value_dim        self.logits = nn.Linear(self.attn_value_dim, 1)        self.self_attn = SelfAttentionModule(vocab_size, attn_query_dim,            self.layer_hidden_dim, attn_value_dim)    def config(self):        return {            "embed_dim": self.embed_dim,            "lstm_hidden_dim": self.lstm_hidden_dim,            "vocab_size": self.vocab_size,            "bidirectional": self.bidirectional,            "num_lstm_layers": self.num_lstm_layers,            "attn_query_dim": self.attn_query_dim,            "attn_value_dim": self.attn_value_dim        }    def forward(self, input_tuple, verbose=False):        x_batch, x_lengths = input_tuple[0], input_tuple[2]        emb_x = self.embed(x_batch)        hidden = self._run_lstm(emb_x, x_lengths)        h = self.self_attn(hidden, x_batch)        x = self.logits(h)        x = torch.sigmoid(x).squeeze()        pred = torch.round(x).type(torch.int)        return x, pred# TODO: computation graph edition for modeling# TODO: transformer