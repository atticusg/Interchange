import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom transformers import BertTokenizerfrom modeling.utils import EmbeddingModule, init_scalingclass ConcatFinalStateModule(nn.Module):    def __init__(self, length_dim=0, bidirectional=True):        if length_dim > 1: raise ValueError(f"Invalid length dim: {length_dim}")        super(ConcatFinalStateModule, self).__init__()        self.length_dim = length_dim        self.bidirectional = bidirectional    def forward(self, hidden):        if self.bidirectional:            hidden_dim = hidden.shape[-1] // 2            if self.length_dim == 0:                forward_out = hidden[-1, :, :hidden_dim]                backward_out = hidden[0, :, hidden_dim:]            elif self.length_dim == 1:                forward_out = hidden[:, -1, :hidden_dim]                backward_out = hidden[:, 0, hidden_dim:]            return torch.cat((forward_out, backward_out), dim=1)        else:            if self.length_dim == 0:                return hidden[-1, :, :]            else:                return hidden[:, -1, :]class LSTMModule(nn.Module):    def __init__(self, output_classes=3, vocab_size=10000,                 tokenizer_vocab_path: str="",                 embed_dim=40, lstm_hidden_dim=20, activation_type="relu",                 bidirectional=True, num_lstm_layers=1, dropout=0.1,                 embed_init_scaling=0.1, fix_embeddings=False,                 batch_first=False, **kwargs):        super(LSTMModule, self).__init__()        self.output_classes = output_classes        self.vocab_size = vocab_size        if tokenizer_vocab_path:            # override vocab size if using bert tokenizer            self.tokenizer_vocab_path = tokenizer_vocab_path            self.tokenizer = BertTokenizer(tokenizer_vocab_path)            self.vocab_size = len(self.tokenizer)        self.embed_dim = embed_dim        self.lstm_hidden_dim = lstm_hidden_dim        self.bidirectional = bidirectional        self.num_lstm_layers = num_lstm_layers        self.dropout = dropout        self.embed_init_scaling = embed_init_scaling        self.fix_embeddings = fix_embeddings        self.activation_type = activation_type        self.batch_first = batch_first        hidden_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim        self.embedding = EmbeddingModule(num_embeddings=self.vocab_size,                                         embedding_dim=embed_dim,                                         fix_weights=fix_embeddings)        self.concat_final_state = ConcatFinalStateModule(            length_dim=(1 if batch_first else 0))        self.lstm_layers = nn.ModuleList()        self.lstm_layers.append(            nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=batch_first,                    bidirectional=bidirectional))        for _ in range(1, num_lstm_layers):            self.lstm_layers.append(                nn.LSTM(lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim,                        lstm_hidden_dim, batch_first=batch_first,                        bidirectional=bidirectional))        self.feed_forward1 = nn.Linear(hidden_dim, hidden_dim)        self.feed_forward2 = nn.Linear(hidden_dim, hidden_dim)        self.logits = nn.Linear(hidden_dim, output_classes)        if activation_type == "relu":            self.activation1, self.activation2 = nn.ReLU(), nn.ReLU()        elif activation_type == "tanh":            self.activation1, self.activation2 = nn.Tanh(), nn.Tanh()        else:            raise ValueError(f"Invalid activation type {activation_type}")        self.lstm_dropout = nn.Dropout(dropout)        self.dropout0 = nn.Dropout(dropout)        self.dropout1 = nn.Dropout(dropout)        self.dropout2 = nn.Dropout(dropout)        init_scaling(self.embed_init_scaling, self.embedding.embedding)    def config(self):        return {            "output_classes": self.output_classes,            "vocab_size": self.vocab_size,            "tokenizer_vocab_path": self.tokenizer_vocab_path,            "embed_dim": self.embed_dim,            "lstm_hidden_dim": self.lstm_hidden_dim,            "bidirectional": self.bidirectional,            "num_lstm_layers": self.num_lstm_layers,            "dropout": self.dropout,            "embed_init_scaling": self.embed_init_scaling,            "fix_embeddings": self.fix_embeddings,            'batch_first': self.batch_first        }    def forward(self, input_tuple, verbose=False):        emb_x = self.embedding(input_tuple)        if len(input_tuple) == 4: # lstm-subphrase variant of MQNLIDataset            hidden = self._run_lstm(emb_x, input_tuple[2])        else:            hidden = self._run_lstm(emb_x)        repr = self.concat_final_state(hidden)        repr = self.dropout0(repr)        output = self.feed_forward1(repr)        output = self.activation1(output)        output = self.dropout1(output)        output = self.feed_forward2(output)        output = self.activation2(output)        output = self.dropout2(output)        output = self.logits(output)        return output    def _run_lstm(self, x, x_lengths=None):        for lstm_layer in self.lstm_layers:            if x_lengths is not None:                x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=self.batch_first)            x, _ = lstm_layer(x)            if x_lengths is not None:                x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=self.batch_first)            x = self.lstm_dropout(x)        return xclass SelfAttentionModule(nn.Module):    def __init__(self, vocab_size=10000, attn_query_dim=5, hidden_dim=40,                 attn_value_dim=5):        super(SelfAttentionModule, self).__init__()        self.query_embed = nn.Embedding(vocab_size, attn_query_dim, padding_idx=0)        self.key_lin_trans = nn.Linear(hidden_dim, attn_query_dim)        self.value_lin_trans = nn.Linear(hidden_dim, attn_value_dim, bias=False)    def forward(self, x, x_batch):        """ Self attention module        :param keys: Hidden vectors        :param x_batch: Raw tokens in idx form        :return:        """        queries = self.query_embed(x_batch)        keys = self.key_lin_trans(x)        a = torch.matmul(queries, torch.transpose(keys, 1, 2))        s = torch.sum(a, 1)        # print("queries:",queries.shape,"\nkeys:", keys.shape, "\na:", a.shape,        #       "\ns:", s.shape)        padding_mask = torch.ne(x_batch, 0).type(torch.float)        exp = torch.mul(torch.exp(s), padding_mask)        attn = torch.nn.functional.normalize(exp, p=1, dim=1).unsqueeze(2)        values = self.value_lin_trans(x)        # print("attn:", attn.shape, "\nvalues:", values.shape)        return torch.sum(attn * values, 1)class LSTMSelfAttnModule(LSTMModule):    def __init__(self, embed_dim=40, lstm_hidden_dim=20, vocab_size=10000,                 bidirectional=True, num_lstm_layers=1, attn_query_dim=5,                 attn_value_dim=5, device=None):        super(LSTMSelfAttnModule, self).__init__(            embed_dim=embed_dim, lstm_hidden_dim=lstm_hidden_dim,            vocab_size=vocab_size, bidirectional=bidirectional,            num_lstm_layers=num_lstm_layers, device=device)        self.attn_query_dim = attn_query_dim        self.attn_value_dim = attn_value_dim        self.logits = nn.Linear(self.attn_value_dim, 1)        self.self_attn = SelfAttentionModule(vocab_size, attn_query_dim,            self.layer_hidden_dim, attn_value_dim)    def config(self):        return {            "embed_dim": self.embed_dim,            "lstm_hidden_dim": self.lstm_hidden_dim,            "vocab_size": self.vocab_size,            "bidirectional": self.bidirectional,            "num_lstm_layers": self.num_lstm_layers,            "attn_query_dim": self.attn_query_dim,            "attn_value_dim": self.attn_value_dim        }    def forward(self, input_tuple, verbose=False):        x_batch, x_lengths = input_tuple[0], input_tuple[2]        emb_x = self.embedding(x_batch)        hidden = self._run_lstm(emb_x, x_lengths)        h = self.self_attn(hidden, x_batch)        x = self.logits(h)        x = torch.sigmoid(x).squeeze()        pred = torch.round(x).type(int)        return x, pred