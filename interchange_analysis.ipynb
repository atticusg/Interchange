{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import db_utils as db\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Callable, Dict, Sequence\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from jupyterthemes import jtplot\n",
    "\n",
    "# # you can select an alternative theme's plot style by name\n",
    "# # onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "# jtplot.style('onedork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data aggregation\n",
    "\n",
    "def aggregate_by_high_node(rs, col_name: str, aggregate: Callable, scale=1, get_argmax=False):\n",
    "    labels = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "              \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "              \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    labels2idx = {l: i for i, l in enumerate(labels)}\n",
    "    baseline = [None] * len(labels)\n",
    "    trained = [None] * len(labels)\n",
    "    if get_argmax:\n",
    "        if not aggregate == max:\n",
    "            raise ValueError(\"Can only get argmax if aggreate is max\")\n",
    "        argmax = [None] * len(labels)\n",
    "    \n",
    "    if aggregate not in {max, min, sum}:\n",
    "        raise ValueError(\"Does not support this type of aggregation\")\n",
    "    \n",
    "    for row in rs:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "        low_node = abstraction[1][0]\n",
    "\n",
    "        is_baseline = \"baseline\" in row[\"model_path\"]\n",
    "        i = labels2idx[high_node]\n",
    "        \n",
    "        if col_name.endswith(\"s\"):\n",
    "            values = json.loads(row[col_name])\n",
    "            value = aggregate(values)\n",
    "        else:\n",
    "            value = row[col_name]\n",
    "        \n",
    "        value *= scale\n",
    "        \n",
    "        if is_baseline:\n",
    "            if baseline[i] is None:\n",
    "                baseline[i] = value\n",
    "                if get_argmax:\n",
    "                    argmax[i] = low_node[-1]\n",
    "            else:\n",
    "                if get_argmax:\n",
    "                    if value > baseline[i]:\n",
    "                        baseline[i] = value\n",
    "                        argmax[i] = low_node[-1]\n",
    "                else:\n",
    "                    baseline[i] = aggregate([baseline[i], value])\n",
    "                \n",
    "        else:\n",
    "            if trained[i] is None:\n",
    "                trained[i] = value\n",
    "            else:\n",
    "                trained[i] = aggregate([trained[i], value])\n",
    "    \n",
    "    if get_argmax:\n",
    "        return labels, baseline, trained, argmax\n",
    "    else:\n",
    "        return labels, baseline, trained\n",
    "    \n",
    "def aggregate_bert_data_by_layer(rs, col_name, high_nodes=None, aggregate=None, scale=1):\n",
    "    low_nodes = [f\"bert_layer_{i}\" for i in range(11)]\n",
    "    low_node_to_idx = {l: i for i, l in enumerate(low_nodes)}\n",
    "    if high_nodes is None:\n",
    "        high_nodes = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "                      \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "                      \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    high_node_to_idx = {l: i for i, l in enumerate(high_nodes)}\n",
    "    res = [[0] * len(low_nodes) for _ in high_nodes]\n",
    "\n",
    "    for row in rs:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "        low_node = abstraction[1][0]\n",
    "\n",
    "        if \"baseline\" in row[\"model_path\"]: continue\n",
    "        \n",
    "        idx = high_node_to_idx[high_node]\n",
    "        if col_name.endswith(\"s\"):\n",
    "            values = json.loads(row[col_name])\n",
    "            value = aggregate(values)\n",
    "        else:\n",
    "            value = row[col_name]\n",
    "        \n",
    "        value *= scale\n",
    "        \n",
    "        val_idx = low_node_to_idx[low_node]\n",
    "        res[idx][val_idx] = value\n",
    "        \n",
    "            \n",
    "    return [f\"layer_{i}\" for i in range(11)], res, high_nodes\n",
    "\n",
    "def aggregate_lstm_data(rs, col_name, aggregate=None, scale=1):\n",
    "    labels = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "              \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "              \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    labels2idx = {l: i for i, l in enumerate(labels)}\n",
    "    baseline = [None] * len(labels)\n",
    "    trained = [None] * len(labels)\n",
    "    for row in rs:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "        is_baseline = \"baseline\" in row[\"model_path\"]\n",
    "        i = labels2idx[high_node]\n",
    "        if col_name.endswith(\"s\"):\n",
    "            values = json.loads(row[col_name])\n",
    "            value = aggregate(values)\n",
    "        else:\n",
    "            value = row[col_name]\n",
    "        \n",
    "        value *= scale\n",
    "        \n",
    "        if is_baseline:\n",
    "            baseline[i] = value if baseline[i] is None else aggregate([baseline[i], value])\n",
    "        else:\n",
    "            trained[i] = value if trained[i] is None else aggregate([trained[i], value])\n",
    "            \n",
    "    return labels, baseline, trained\n",
    "\n",
    "\n",
    "def aggregate_data_by_ratio(rows, numerator_cols: Sequence[str], denominator_cols: Sequence[str], aggregate: Callable):\n",
    "    labels = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "              \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "              \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    labels2idx = {l: i for i, l in enumerate(labels)}\n",
    "    res = [None] * len(labels)\n",
    "    \n",
    "    if aggregate not in {max, min, sum}:\n",
    "        raise ValueError(\"Does not support this type of aggregation\")\n",
    "    \n",
    "    for row in rows:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "\n",
    "        i = labels2idx[high_node]\n",
    "\n",
    "        numerator_values = sum(np.array(json.loads(row[c])) for c in numerator_cols)\n",
    "        denominator_values = sum(np.array(json.loads(row[c])) for c in denominator_cols)\n",
    "        values = [x / y if y != 0 else 0 for x, y in zip(numerator_values, denominator_values)]\n",
    "        value = aggregate(values)\n",
    "\n",
    "        \n",
    "        if res[i] is None:\n",
    "            res[i] = value\n",
    "        else:\n",
    "            res[i] = aggregate([res[i], value])\n",
    "    \n",
    "    return labels, res\n",
    "\n",
    "def max_data_by_normalized_columns(rows, cols, maximizing_cols):\n",
    "    labels = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "              \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "              \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    labels2idx = {l: i for i, l in enumerate(labels)}\n",
    "    cols2idx = {c: i for i, c in enumerate(cols)}\n",
    "    res = [None] * len(labels)\n",
    "\n",
    "    for row in rows:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "        i = labels2idx[high_node]\n",
    "        \n",
    "        totals = sum(np.array(json.loads(row[c])) for c in cols)\n",
    "        ratios = [np.array(json.loads(row[c])) / totals for c in cols]\n",
    "        \n",
    "        # print(\"totals\", totals)\n",
    "        # print(\"ratios\", ratios)\n",
    "        \n",
    "        max_vals = sum(ratios[cols2idx[c]] for c in maximizing_cols)\n",
    "        argmax_i = np.argmax(max_vals)\n",
    "        # print(\"max_vals\", max_vals)\n",
    "        # print(\"argmax_i\", argmax_i)\n",
    "\n",
    "        max_val = max_vals[argmax_i]\n",
    "        res_ratios = np.array([r[argmax_i] for r in ratios])\n",
    "        \n",
    "        if res[i] is None:\n",
    "            res[i] = res_ratios\n",
    "        else:\n",
    "            orig_max_val = sum(res[i][cols2idx[c]] for c in maximizing_cols)\n",
    "            if max_val > orig_max_val:\n",
    "                res[i] = res_ratios\n",
    "        \n",
    "    res = [np.array([r[c] for r in res]) for c in range(len(cols))]\n",
    "    return labels, res\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools for plotting\n",
    "def multi_line_plot(x_labels: Sequence[str], \n",
    "                    data: Sequence[Sequence],\n",
    "                    line_labels: Sequence[str],\n",
    "                    y_label: str, \n",
    "                    title: str):\n",
    "    \n",
    "    xs = np.arange(len(x_labels))  # the label locations\n",
    "    assert len(data) == len(line_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    \n",
    "    lines = []\n",
    "    for i, (ys, label) in enumerate(zip(data, line_labels)):\n",
    "        line = ax.plot(xs, ys, label=label)\n",
    "        lines.append(line)\n",
    "    \n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def multi_bar_plot(x_labels: Sequence[str], \n",
    "                   data: Sequence[Sequence],\n",
    "                   category_labels: Sequence[str],\n",
    "                   bar_y_label: str, \n",
    "                   title: str, \n",
    "                   category_styles: Dict={},\n",
    "                   line_y_label: str=None,\n",
    "                   width: int=0.7, \n",
    "                   bar_label_fmt: str='{:.1%}'):\n",
    "    \n",
    "    xs = np.arange(len(x_labels))  # the label locations\n",
    "    assert len(data) == len(category_labels)\n",
    "\n",
    "    bar_labels = [k for k in category_labels if category_styles.get(k, \"bar\") == \"bar\"]\n",
    "    line_labels = [k for k in category_labels if k in category_styles and category_styles[k] == \"line\"]\n",
    "    \n",
    "    bar_data = [data for data, label in zip(data, category_labels) if label in bar_labels]\n",
    "    line_data = [data for data, label in zip(data, category_labels) if label in line_labels]\n",
    "\n",
    "    bar_width = width / len(bar_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    bars = []\n",
    "    for i, (ys, label) in enumerate(zip(bar_data, bar_labels)):\n",
    "        bar_positions = xs - width/2 + bar_width * i\n",
    "        bar = ax.bar(bar_positions, ys, bar_width, label=label, align=\"edge\")\n",
    "        bars.append(bar)\n",
    "    \n",
    "    if not line_y_label:\n",
    "        lines = []\n",
    "        for i, (ys, label) in enumerate(zip(line_data, line_labels)):\n",
    "            color_id = len(bars) + i\n",
    "            line = ax.plot(xs, ys, color=f\"C{color_id}\", label=label)\n",
    "\n",
    "        # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "        ax.set_ylabel(bar_y_label)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(xs)\n",
    "        ax.set_xticklabels(x_labels)\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.set_ylabel(bar_y_label)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(xs)\n",
    "        ax.set_xticklabels(x_labels)\n",
    "        ax.legend()\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        lines = []\n",
    "        for i, (ys, label) in enumerate(zip(line_data, line_labels)):\n",
    "            color_id = len(bars) + i\n",
    "            line = ax2.plot(xs, ys, color=f\"C{color_id}\", label=label)\n",
    "        y_axis_color = f\"C{len(bars)}\"\n",
    "        ax2.set_ylabel(line_y_label, color=y_axis_color)\n",
    "        ax2.tick_params(axis='y', labelcolor=y_axis_color)\n",
    "        \n",
    "        ax2.legend()\n",
    "\n",
    "\n",
    "    def autolabel(rects):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(bar_label_fmt.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    for bar in bars:\n",
    "        autolabel(bar)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def stacked_bar_plot(x_labels: Sequence[str], \n",
    "                       data: Sequence[Sequence],\n",
    "                       category_labels: Sequence[str],\n",
    "                       bar_y_label: str, \n",
    "                       title: str, \n",
    "                       bar_width: int=0.5, \n",
    "                       bar_label_fmt: str='{:.1%}'):\n",
    "    xs = np.arange(len(x_labels))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    bars = []\n",
    "    bottom = np.array([0.0 for _ in xs])\n",
    "    for i, (ys, label) in enumerate(zip(data, category_labels)):\n",
    "        bar = ax.bar(xs, ys, bar_width, bottom=bottom, label=label)\n",
    "        bars.append(bar)\n",
    "        bottom += np.array(ys)\n",
    "    \n",
    "    ax.set_ylabel(bar_y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "        for rect in rects:\n",
    "            x = rect.get_x() + rect.get_width() / 2\n",
    "            height = rect.get_height()\n",
    "            y = rect.get_y() + height\n",
    "            \n",
    "            ax.annotate(bar_label_fmt.format(height),\n",
    "                        xy=(x, y),\n",
    "                        xytext=(0, -13),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    for bar in bars:\n",
    "        autolabel(bar)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def multi_stacked_bar_plot(x_labels: Sequence[str],\n",
    "                           data: Sequence[Sequence],\n",
    "                           series_labels: Sequence[str],\n",
    "                           category_labels: Sequence[str],\n",
    "                           bar_y_label: str, \n",
    "                           title: str, \n",
    "                           bar_width: int=0.7, \n",
    "                           bar_label_fmt: str='{:.1%}'):\n",
    "    x_range = np.arange(len(x_labels))\n",
    "    one_bar_width = bar_width / len(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    bars = []\n",
    "    \n",
    "    if len(category_labels) == 5:\n",
    "        colors = [\"#5dade2\",  \"#E74C3C\", \"#85C1E9\", \"#F1948A\", \"#A6ACAF\"]\n",
    "    elif\n",
    "        \n",
    "\n",
    "    for i, (series_data, series_label) in enumerate(zip(data, series_labels)):\n",
    "        xs = x_range - bar_width/2 + one_bar_width * i\n",
    "        bottom = np.array([0.0 for _ in xs])\n",
    "        for j, (ys, category_label) in enumerate(zip(series_data, category_labels)):\n",
    "            if len(category_labels) == 5:\n",
    "                hatch = \"///\" if j == 0 or j == 1 else \"\"\n",
    "            else:\n",
    "                hatch = \"\"\n",
    "                \n",
    "            if i == 0:\n",
    "                bar = ax.bar(xs, ys, one_bar_width-0.05, bottom=bottom, label=category_label, align=\"edge\", hatch=hatch, color=colors[j])\n",
    "            else:\n",
    "                bar = ax.bar(xs, ys, one_bar_width-0.05, bottom=bottom, align=\"edge\", hatch=hatch, color=colors[j])\n",
    "            bars.append(bar)\n",
    "            bottom += np.array(ys)\n",
    "    \n",
    "    ax.set_ylabel(bar_y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x_range)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "        for rect in rects:\n",
    "            x = rect.get_x() + rect.get_width() / 2\n",
    "            height = rect.get_height()\n",
    "            y = rect.get_y() + height\n",
    "            \n",
    "            ax.annotate(bar_label_fmt.format(height),\n",
    "                        xy=(x, y),\n",
    "                        xytext=(0, -13),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    if bar_label_fmt:\n",
    "        for bar in bars:\n",
    "            autolabel(bar)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hard_data = db.select(\"experiment_data/bert/bert-hard.db\", \"results\", cond_dict={\"status\": 3})\n",
    "print(f\"got {len(bert_hard_data)} rows from bert_hard_data\")\n",
    "lstm_hard_data = db.select(\"experiment_data/lstm/lstm-hard.db\", \"results\", cond_dict={\"status\": 3})\n",
    "print(f\"got {len(lstm_hard_data)} rows from lstm_hard_data\")\n",
    "\n",
    "bert_easy_data = db.select(\"experiment_data/bert/bert-easy.db\", \"results\", cond_dict={\"status\": 3})\n",
    "print(f\"got {len(bert_easy_data)} rows from bert_easy_data\")\n",
    "lstm_easy_data = db.select(\"experiment_data/lstm/lstm-easy.db\", \"results\", cond_dict={\"status\": 1})\n",
    "print(f\"got {len(lstm_easy_data)} rows from lstm_easy_data\")\n",
    "\n",
    "bert_baseline_data = db.select(\"experiment_data/bert/bert-baseline.db\", \"results\", cond_dict={\"status\": 1})\n",
    "lstm_baseline_data = db.select(\"experiment_data/lstm/lstm-baseline.db\", \"results\", cond_dict={\"status\": 2})\n",
    "\n",
    "# lstm_hard_data = db.select(\"experiment_data/sep/nov2-500.db\", \"results\", cond_dict={\"status\": 4}, like={\"abstraction\": \"%lstm_0%\"})\n",
    "# print(len(old_lstm_easy_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/intervention.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{weight}(x, x', V) = \\text{Count}^{-1}[V' : y_{V' \\leftarrow V'(x')}(x) = y_{V \\leftarrow V(x')} (x)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema of interchange experiment\n",
    "\n",
    "<img src=\"figures/interchange_experiment.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot by outcome category\n",
    "\n",
    "<img src=\"figures/outcome_categories_colored.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_normalized_column_data(data):\n",
    "#     all_cols = [f\"res_{i}_counts\" for i in range(8)]\n",
    "#     maximizing_cols = [\"res_3_counts\"]\n",
    "\n",
    "#     labels, res = max_data_by_normalized_columns(data, all_cols, maximizing_cols)\n",
    "\n",
    "#     non_causal = sum(res[4:8])\n",
    "\n",
    "#     data = res[:4] + [non_causal]\n",
    "#     category_labels = [\"!base, !success\", \"!base, success\", \"base, !success\", \"base, success\", \"non-causal\"]\n",
    "#     return labels, data, category_labels\n",
    "\n",
    "\n",
    "def get_normalized_column_data(data):\n",
    "    all_cols = [f\"res_{i}_counts\" for i in range(8)]\n",
    "    maximizing_cols = [\"res_3_counts\"]\n",
    "\n",
    "    labels, res = max_data_by_normalized_columns(data, all_cols, maximizing_cols)\n",
    "    \n",
    "\n",
    "    incorrect = sum([res[0], res[1], res[4], res[5]])\n",
    "\n",
    "    data = [res[3], res[2], res[7],  res[6], incorrect]\n",
    "    category_labels = [\"causal, successful\",  \"causal, unsuccessful\", \"non-causal, successful\",  \"non-causal, unsuccessful\", \"base result incorrect\"]\n",
    "    return labels, data, category_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, bert_category_data, category_labels = get_normalized_column_data(bert_hard_data)\n",
    "_, lstm_category_data, _ = get_normalized_column_data(lstm_hard_data)\n",
    "_, lstm_baseline_category_data, _ = get_normalized_column_data(lstm_baseline_data)\n",
    "_, bert_baseline_category_data, _ = get_normalized_column_data(bert_baseline_data)\n",
    "print(bert_baseline_category_data)\n",
    "print(category_labels)\n",
    "\n",
    "# stacked_bar_plot(labels, bert_category_data, category_labels, \"ratio\", \"test\")\n",
    "multi_stacked_bar_plot(labels, [bert_baseline_category_data, lstm_category_data, bert_category_data], \n",
    "                       [\"bert_baseline\", \"lstm\", \"bert\"], \n",
    "                       category_labels, \n",
    "                       \"ratio\", \"Breakdown of outcomes of models trained on hard datasets (left to right: LSTM baseline, BERT baseline, LSTM, BERT)\", \n",
    "                       bar_width=0.7,\n",
    "                       bar_label_fmt=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, bert_category_data, category_labels = get_normalized_column_data(bert_easy_data)\n",
    "_, lstm_category_data, category_labels = get_normalized_column_data(lstm_easy_data)\n",
    "_, lstm_baseline_category_data, _ = get_normalized_column_data(lstm_baseline_data)\n",
    "\n",
    "\n",
    "# stacked_bar_plot(labels, bert_category_data, category_labels, \"ratio\", \"test\")\n",
    "multi_stacked_bar_plot(labels, [lstm_baseline_category_data, lstm_category_data, bert_category_data], \n",
    "                       [\"baseline\", \"lstm\", \"bert\"], \n",
    "                       category_labels, \n",
    "                       \"ratio\", \"Breakdown of outcomes of models trained on easy datasets (left to right: Baseline, LSTM, BERT)\", \n",
    "                       bar_width=0.7,\n",
    "                       bar_label_fmt=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_causal_column_data(data):\n",
    "    all_cols = [f\"res_{i}_counts\" for i in range(8)]\n",
    "    maximizing_cols = [\"res_3_counts\"]\n",
    "\n",
    "    labels, res = max_data_by_normalized_columns(data, all_cols, maximizing_cols)\n",
    "\n",
    "    total = sum(res[:4])\n",
    "    \n",
    "    data = [r / total for r in res]\n",
    "\n",
    "    category_labels = [\"!base_eq, !success\", \"!base_eq, success\", \"base_eq, !success\", \"base_eq, success\"]\n",
    "    return labels, data, category_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, bert_category_data, category_labels = get_normalized_causal_column_data(bert_hard_data)\n",
    "_, lstm_category_data, category_labels = get_normalized_causal_column_data(lstm_hard_data)\n",
    "_, baseline_category_data, _ = get_normalized_causal_column_data(lstm_baseline_data)\n",
    "\n",
    "\n",
    "# stacked_bar_plot(labels, bert_category_data, category_labels, \"ratio\", \"test\")\n",
    "multi_stacked_bar_plot(labels, [baseline_category_data, lstm_category_data, bert_category_data], \n",
    "                       [\"baseline\", \"lstm\", \"bert\"],\n",
    "                       category_labels, \n",
    "                       \"ratio\", \"Breakdown of CAUSAL outcomes of models trained on hard datasets (left to right: Baseline, LSTM, BERT)\", bar_width=0.7, bar_label_fmt=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, bert_category_data, category_labels = get_normalized_causal_column_data(bert_easy_data)\n",
    "_, lstm_category_data, category_labels = get_normalized_causal_column_data(lstm_easy_data)\n",
    "_, baseline_category_data, _ = get_normalized_causal_column_data(lstm_baseline_data)\n",
    "\n",
    "# stacked_bar_plot(labels, bert_category_data, category_labels, \"ratio\", \"test\")\n",
    "# stacked_bar_plot(labels, bert_category_data, category_labels, \"ratio\", \"test\")\n",
    "multi_stacked_bar_plot(labels, [baseline_category_data, lstm_category_data, bert_category_data], \n",
    "                       [\"baseline\", \"lstm\", \"bert\"],\n",
    "                       category_labels, \n",
    "                       \"ratio\", \"Breakdown of CAUSAL outcomes of models trained on hard datasets (left to right: Baseline, LSTM, BERT)\", \n",
    "                       bar_width=0.7, bar_label_fmt=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Intervention Success Rates\n",
    "\n",
    "$$\\text{Success rate} = \\frac{Count[\\text{intervention correct, low base output correct, intervention impactful}]}{Count[\\text{low base output correct, intervention impactful}]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_success_rate(baseline_data, lstm_data, bert_data, title):\n",
    "    numerator_cols = [\"res_3_counts\"]\n",
    "    denominator_cols = [\"res_2_counts\", \"res_3_counts\"]\n",
    "\n",
    "    labels, bert_hard_success_rates = aggregate_data_by_ratio(bert_data, numerator_cols, denominator_cols, max)\n",
    "    _, lstm_hard_success_rates = aggregate_data_by_ratio(lstm_data, numerator_cols, denominator_cols, max)\n",
    "    _, lstm_baseline_success_rates = aggregate_data_by_ratio(baseline_data, numerator_cols, denominator_cols, max)\n",
    "\n",
    "    data_for_plot = [lstm_hard_success_rates, bert_hard_success_rates]\n",
    "    data_labels = [\"LSTM\", \"Bert\"]\n",
    "    multi_bar_plot(labels, data_for_plot, data_labels, bar_y_label=\"success rate\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_success_rate(lstm_baseline_data, lstm_easy_data, bert_easy_data, \"Intervention success rate on correct inputs and causal interventions, models trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_success_rate(lstm_baseline_data, lstm_hard_data, bert_hard_data, \"Intervention success rate on correct inputs and causal interventions, models trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaxed success rate\n",
    "\n",
    "$$\\text{Relaxed success rate} = \\frac{Count[\\text{intervention correct, intervention impactful}]}{Count[\\text{intervention impactful}]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relaxed_success_rate(baseline_data, lstm_data, bert_data, title):\n",
    "    numerator_cols = [\"res_1_counts\", \"res_3_counts\"]\n",
    "    denominator_cols = [\"res_0_counts\", \"res_1_counts\", \"res_2_counts\", \"res_3_counts\"]\n",
    "    labels, bert_success_rates = aggregate_data_by_ratio(bert_data, numerator_cols, denominator_cols, max)\n",
    "    _, lstm_success_rates = aggregate_data_by_ratio(lstm_data, numerator_cols, denominator_cols, max)\n",
    "    _, baseline_success_rates = aggregate_data_by_ratio(baseline_data, numerator_cols, denominator_cols, max)\n",
    "\n",
    "    data_for_plot = [baseline_success_rates, lstm_success_rates, bert_success_rates]\n",
    "    data_labels = [\"Baseline\", \"LSTM\", \"Bert\"]\n",
    "    multi_bar_plot(labels, data_for_plot, data_labels, bar_y_label=\"relaxed success rate\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relaxed_success_rate(lstm_baseline_data, lstm_easy_data, bert_easy_data, \"Maximum success rate on all inputs, trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relaxed_success_rate(lstm_baseline_data, lstm_hard_data, bert_hard_data, \"Maximum success rate on all inputs, trained on hard dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rate(baseline_data, lstm_data, bert_data, title):\n",
    "    numerator_cols = [\"res_6_counts\"]\n",
    "    denominator_cols = [\"res_6_counts\", \"res_7_counts\"]\n",
    "    labels, bert_success_rates = aggregate_data_by_ratio(bert_data, numerator_cols, denominator_cols, max)\n",
    "    _, lstm_success_rates = aggregate_data_by_ratio(lstm_data, numerator_cols, denominator_cols, max)\n",
    "    _, baseline_success_rates = aggregate_data_by_ratio(baseline_data, numerator_cols, denominator_cols, max)\n",
    "\n",
    "    data_for_plot = [baseline_success_rates, lstm_success_rates, bert_success_rates]\n",
    "    data_labels = [\"Baseline\", \"LSTM\", \"Bert\"]\n",
    "    multi_bar_plot(labels, data_for_plot, data_labels, bar_y_label=\"relaxed success rate\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_success_rate(lstm_baseline_data, lstm_easy_data, bert_easy_data, \"Maximum success rate on all inputs, trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_success_rate(lstm_baseline_data, lstm_hard_data, bert_hard_data, \"Maximum success rate on all inputs, trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error base result success rate\n",
    "$$\\text{Error base result success rate} = \\frac{Count[\\text{intervention correct, low base result incorrect, intervention impactful}]}{Count[\\text{low base result incorrect, intervention impactful}]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_base_result_success_rate(baseline_data, lstm_data, bert_data, title):\n",
    "    numerator_cols = [\"res_1_counts\"]\n",
    "    denominator_cols = [\"res_0_counts\", \"res_1_counts\"]\n",
    "    labels, bert_success_rates = aggregate_data_by_ratio(bert_data, numerator_cols, denominator_cols, max)\n",
    "    _, lstm_success_rates = aggregate_data_by_ratio(lstm_data, numerator_cols, denominator_cols, max)\n",
    "    _, baseline_success_rates = aggregate_data_by_ratio(baseline_data, numerator_cols, denominator_cols, max)\n",
    "\n",
    "    data_for_plot = [baseline_success_rates, lstm_success_rates, bert_success_rates]\n",
    "    data_labels = [\"baseline\", \"LSTM\", \"Bert\"]\n",
    "    multi_bar_plot(labels, data_for_plot, data_labels, bar_y_label=\"error base result success rate\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_base_result_success_rate(lstm_baseline_data, lstm_easy_data, bert_easy_data, \"Maximum success rate on inputs wrongly classified by model, trained on easy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_base_result_success_rate(lstm_baseline_data, lstm_hard_data, bert_hard_data, \"Maximum success rate on inputs wrongly classified by model, trained on hard dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, bert_baseline_eq_rate, bert_eq_rate, argmax = aggregate_by_high_node(bert_data, \"interv_eq_rates\", aggregate=max, get_argmax=True)\n",
    "# _, lstm_baseline_eq_rate, lstm_eq_rate = arrange_lstm_data(lstm_data, \"interv_eq_rates\", aggregate=max)\n",
    "# _, _, causal_ratio = aggregate_by_high_node(bert_data, \"effective_ratios\", aggregate=max, scale=250000)\n",
    "\n",
    "# data_for_plot = [bert_baseline_eq_rate, lstm_baseline_eq_rate, lstm_eq_rate, bert_eq_rate, causal_ratio]\n",
    "# data_labels = [\"Bert Baseline\", \"LSTM Baseline\", \"LSTM\", \"Bert\", \"Causal interchanges\"]\n",
    "# category_styles = {\"Causal interchanges\": \"line\"}\n",
    "# multi_bar_plot(labels, data_for_plot, data_labels, \n",
    "#                category_styles=category_styles,\n",
    "#                bar_y_label=\"#successful / #causal\", \n",
    "#                line_y_label=\"#causal (#total is 250000)\",\n",
    "#                title=\"Maximum intervention success rate among all low-level locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success rate by bert layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_nodes, values, high_nodes = aggregate_bert_data_by_layer(bert_data, \"interv_eq_rates\", high_nodes=None, aggregate=max)\n",
    "multi_line_plot(low_nodes, values, high_nodes, y_label=\"#successful / #causal\", title=\"Intervention success rate by bert layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique size aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max\n",
    "labels, _, lstm_clq_size = aggregate_lstm_data(lstm_easy_data, \"max_clique_sizes\", aggregate=max)\n",
    "_, _, bert_clq_size = aggregate_by_high_node(bert_easy_data, \"max_clique_sizes\", aggregate=max)\n",
    "_, _, lstm_baseline_clq_size = aggregate_lstm_data(lstm_baseline_data, \"max_clique_sizes\", aggregate=max)\n",
    "\n",
    "print(lstm_baseline_clq_size)\n",
    "print(lstm_clq_size)\n",
    "print(bert_clq_size)\n",
    "\n",
    "data_for_plot = [lstm_clq_size, bert_clq_size]\n",
    "data_labels = [\"LSTM\", \"Bert\"]\n",
    "multi_bar_plot(labels, data_for_plot, data_labels, \n",
    "               bar_y_label=\"#successful / #causal\", \n",
    "               title=\"Maximum clique sizes for models trained on the hard dataset\",\n",
    "               bar_label_fmt='{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max\n",
    "labels, _, lstm_clq_size = aggregate_lstm_data(lstm_hard_data, \"max_clique_sizes\", aggregate=max)\n",
    "_, _, bert_clq_size = aggregate_by_high_node(bert_hard_data, \"max_clique_sizes\", aggregate=max)\n",
    "_, _, lstm_baseline_clq_size = aggregate_lstm_data(lstm_baseline_data, \"max_clique_sizes\", aggregate=max)\n",
    "\n",
    "print(lstm_baseline_clq_size)\n",
    "print(lstm_clq_size)\n",
    "print(bert_clq_size)\n",
    "\n",
    "data_for_plot = [lstm_clq_size, bert_clq_size]\n",
    "data_labels = [\"LSTM\", \"Bert\"]\n",
    "multi_bar_plot(labels, data_for_plot, data_labels, \n",
    "               bar_y_label=\"#successful / #causal\", \n",
    "               title=\"Maximum clique sizes for models trained on the hard dataset\",\n",
    "               bar_label_fmt='{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_nodes, values, high_nodes = arrange_data_by_bert_layer(bert_data, \"max_clique_sizes\", high_nodes=None, aggregate=max)\n",
    "multi_line_plot(low_nodes, values, high_nodes, y_label=\"#successful / #causal\", title=\"Max clique size by bert layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_nodes, values, high_nodes = arrange_data_by_bert_layer(bert_data, \"avg_clique_sizes\", high_nodes=None, aggregate=max)\n",
    "multi_line_plot(low_nodes, values, high_nodes, y_label=\"#successful / #causal\", title=\"Average clique size by bert layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg\n",
    "labels, base_eq_rate, trained_eq_rate = arrange_data(rows, \"avg_clique_sizes\", aggregate=max, scale=1)\n",
    "bar_plot(labels, x1=base_eq_rate, x2=trained_eq_rate, x1_label=\"Baseline\", x2_label=\"Trained\", \n",
    "         y_label=\"interv_eq_rate\", title=\"Average clique sizes (max among all intervention locations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum\n",
    "labels, base_eq_rate, trained_eq_rate = arrange_data(rows, \"sum_clique_sizes\", aggregate=max, scale=1)\n",
    "bar_plot(labels, x1=base_eq_rate, x2=trained_eq_rate, x1_label=\"Baseline\", x2_label=\"Trained\", \n",
    "         y_label=\"interv_eq_rate\", title=\"Sum of clique sizes (max among all intervention locations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max clique sizes, left vs right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, _, left_max_clique = arrange_data(rows, \"max_clique_sizes\", aggregate=lambda s: s[0], scale=1)\n",
    "labels, _, right_max_clique = arrange_data(rows, \"max_clique_sizes\", aggregate=lambda s: s[-1], scale=1)\n",
    "bar_plot(labels, x1=left_max_clique, x2=right_max_clique, x1_label=\"Left\", x2_label=\"Right\", \n",
    "         y_label=\"Max clique sizes\", title=\"Max clique sizes, left vs right\", fmt=\"{}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal edge ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, _, trained = arrange_data(rows, \"causal_edge_ratios\", aggregate=max, scale=100)\n",
    "bar_plot(labels, x1=trained, x1_label=\"Trained\", x1_color=\"orange\",\n",
    "         y_label=\"Causal edge ratio %\", title=\"Percentage of edges that are causal within all cliques (max among all intervention locations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, _, trained = arrange_data(rows, \"max_clique_calusal_edge_ratios\", aggregate=max, scale=100)\n",
    "bar_plot(labels, x1=trained, x1_label=\"Trained\", x1_color=\"orange\",\n",
    "         y_label=\"Causal edge ratio %\", title=\"Percentage of edges that are causal within the largest clique (max among all intervention locations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_causal_edge_ratio(rows):\n",
    "    labels = [\"sentence_q\", \"subj_adj\", \"subj_noun\", \"subj\", \"neg\", \"negp\", \n",
    "          \"v_adv\", \"v_verb\", \"v_bar\", \"vp\", \"vp_q\", \n",
    "          \"obj_adj\", \"obj_noun\", \"obj\"]\n",
    "    labels2idx = {l: i for i, l in enumerate(labels)}\n",
    "    trained = [0.0] * len(labels)\n",
    "    for row in rows:\n",
    "        abstraction = json.loads(row[\"abstraction\"])\n",
    "        high_node = abstraction[0]\n",
    "        is_baseline = \"baseline\" in row[\"model_path\"]\n",
    "        if is_baseline: continue\n",
    "        i = labels2idx[high_node]\n",
    "        if col_name.endswith(\"s\"):\n",
    "            values = json.loads(row[\"max_clique_size\"])\n",
    "            \n",
    "        else:\n",
    "            value = row[col_name]\n",
    "        \n",
    "        value *= scale\n",
    "        \n",
    "        if is_baseline:\n",
    "\n",
    "        trained[i] = value\n",
    "            \n",
    "    return labels, baseline, trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify and merge lstm-easy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_lstm_easy_data = db.select(\"experiment_data/lstm/lstm-easy.db\", \"results\", cond_dict={\"status\": 1})\n",
    "# old_lstm_easy_data = db.select(\"experiment_data/sep/nov2-500.db\", \"results\", cond_dict={\"status\": 4}, like={\"abstraction\": \"%lstm_0%\"})\n",
    "\n",
    "# numerator_cols = [\"res_1_counts\", \"res_3_counts\"]\n",
    "# denominator_cols = [\"res_0_counts\", \"res_1_counts\", \"res_2_counts\", \"res_3_counts\"]\n",
    "# labels, new_lstm_easy_interv_eq_rates = aggregate_lstm_data_by_ratio(new_lstm_easy_data, numerator_cols, denominator_cols, max)\n",
    "# _, _, old_lstm_easy_interv_eq_rates = aggregate_lstm_data(old_lstm_easy_data, \"interv_eq_rates\", max)\n",
    "\n",
    "# print(new_lstm_easy_interv_eq_rates)\n",
    "# print(old_lstm_easy_interv_eq_rates)\n",
    "\n",
    "# merge graph data into new lstm_easy\n",
    "\n",
    "# for row in old_lstm_easy_data:\n",
    "#     update_dict = {\n",
    "#         \"graph_save_paths\": row[\"graph_save_paths\"],\n",
    "#         \"max_clique_sizes\": row[\"max_clique_sizes\"],\n",
    "#         \"avg_clique_sizes\": row[\"avg_clique_sizes\"],\n",
    "#         \"sum_clique_sizes\": row[\"sum_clique_sizes\"],\n",
    "#         \"clique_counts\": row[\"clique_counts\"],\n",
    "#         \"visualize_save_paths\": row[\"visualize_save_paths\"]\n",
    "#     }\n",
    "#     db.add_cols(\"experiment_data/lstm/lstm-easy.db\", \"results\", update_dict,)\n",
    "#     db.update(\"experiment_data/lstm/lstm-easy.db\", \"results\", update_dict, id=(row[\"id\"]+1)//2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
